{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The notebook shows how to extract the segmentation map for the ships, augment the images and train a simple DNN model to detect them. A few additional tweaks like balancing the ship-count out a little better have been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "We might want to adjust these later (or do some hyperparameter optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EDGE_CROP = 16\n",
    "NB_EPOCHS = 30\n",
    "GAUSSIAN_NOISE = 0.1\n",
    "UPSAMPLE_MODE = 'SIMPLE'\n",
    "# downsampling inside the network\n",
    "NET_SCALING = None\n",
    "# downsampling in preprocessing\n",
    "IMG_SCALING = (1, 1)\n",
    "# number of validation images to use\n",
    "VALID_IMG_COUNT = 400\n",
    "# maximum number of steps_per_epoch in training\n",
    "MAX_TRAIN_STEPS = 200\n",
    "AUGMENT_BRIGHTNESS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_PLOT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20 2019-07-27 14:54:24 : Logging started\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "\n",
    "# Set level\n",
    "logger.setLevel(logging.INFO)\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter\n",
    "FORMAT = \"%(levelno)-2s %(asctime)s : %(message)s\"\n",
    "DATE_FMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "\n",
    "# Create handler and assign\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]\n",
    "logging.info(\"Logging started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Sci stack\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn\n",
    "from skimage.io import imread\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import montage\n",
    "from skimage.morphology import label\n",
    "from sklearn.model_selection import train_test_split\n",
    "montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n",
    "\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ship_dir = '../input'\n",
    "# train_image_dir = os.path.join(ship_dir, 'train_v2')\n",
    "# test_image_dir = os.path.join(ship_dir, 'test_v2')\n",
    "import gc; gc.enable() # memory is tight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_rle_encode(img):\n",
    "    labels = label(img[:, :, 0])\n",
    "    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set memory limit of TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "session = tf.Session(config=config)\n",
    "import tensorflow.keras.backend as K\n",
    "K.set_session(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9266147303871762699, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 14548421494380153483\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 7589507541014127374\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231723 masks found\n",
      "192556\n"
     ]
    }
   ],
   "source": [
    "DIR_INPUT = Path(r\"~/DATA\").expanduser()\n",
    "assert DIR_INPUT.exists()\n",
    "PATH_CSV = DIR_INPUT / 'train_ship_segmentations_v2.csv'\n",
    "assert PATH_CSV.exists()\n",
    "DIR_IMAGES = DIR_INPUT / 'images'\n",
    "assert DIR_IMAGES.exists()\n",
    "masks = pd.read_csv(PATH_CSV)\n",
    "print(masks.shape[0], 'masks found')\n",
    "print(masks['ImageId'].value_counts().shape[0])\n",
    "DIR_WEIGHTS = DIR_INPUT / 'weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "title": "Align the df with the actual sampled data"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20 2019-07-27 14:56:33 : Resampled df to match existing images, 231723 records\n"
     ]
    }
   ],
   "source": [
    "masks\n",
    "DIR_IMAGES.joinpath('teas').exists()\n",
    "masks['exists'] = masks['ImageId'].apply(lambda image_id: DIR_IMAGES.joinpath(image_id).exists())\n",
    "# r = masks.head(10)\n",
    "masks = masks[masks['exists']]\n",
    "logging.info(\"Resampled df to match existing images, {} records\".format(len(masks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure encode/decode works\n",
    "Given the process\n",
    "$$  RLE_0 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_0 \\stackrel{Encode}{\\longrightarrow} RLE_1 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_1 $$\n",
    "We want to check if/that\n",
    "$ \\textrm{Image}_0 \\stackrel{?}{=} \\textrm{Image}_1 $\n",
    "We could check the RLEs as well but that is more tedious. Also depending on how the objects have been labeled we might have different counts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_PLOT:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    rle_0 = masks.query('ImageId==\"00021ddc3.jpg\"')['EncodedPixels']\n",
    "    img_0 = masks_as_image(rle_0)\n",
    "    ax1.imshow(img_0[:, :, 0])\n",
    "    ax1.set_title('Image$_0$')\n",
    "    rle_1 = multi_rle_encode(img_0)\n",
    "    img_1 = masks_as_image(rle_1)\n",
    "    ax2.imshow(img_1[:, :, 0])\n",
    "    ax2.set_title('Image$_1$')\n",
    "    print('Check Decoding->Encoding',\n",
    "          'RLE_0:', len(rle_0), '->',\n",
    "          'RLE_1:', len(rle_1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into training and validation groups\n",
    "We stratify by the number of boats appearing so we have nice balances in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20 2019-07-27 14:56:40 : Unique records: 191289\n",
      "20 2019-07-27 14:56:40 : Total records: 231723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "# some files are too small/corrupt\n",
    "masks['ImageId'].apply(lambda image_id: DIR_IMAGES.joinpath(image_id).exists())\n",
    "\n",
    "unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda image_id: os.stat(str(DIR_IMAGES.joinpath(image_id))).st_size/1024)\n",
    "unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\n",
    "unique_img_ids['file_size_kb'].hist()\n",
    "# plt.show()\n",
    "masks.drop(['ships'], axis=1, inplace=True)\n",
    "unique_img_ids.sample(5)\n",
    "logging.info(\"Unique records: {}\".format(len(unique_img_ids)))\n",
    "logging.info(\"Total records: {}\".format(len(masks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161048 training masks\n",
      "69034 validation masks\n"
     ]
    }
   ],
   "source": [
    "train_ids, valid_ids = train_test_split(unique_img_ids,\n",
    "                 test_size = 0.3, \n",
    "                 stratify = unique_img_ids['ships'])\n",
    "train_df = pd.merge(masks, train_ids)\n",
    "valid_df = pd.merge(masks, valid_ids)\n",
    "print(train_df.shape[0], 'training masks')\n",
    "print(valid_df.shape[0], 'validation masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Number of Ship Images\n",
    "Here we examine how often ships appear and replace the ones without any ships with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc42857aa58>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['ships'].hist()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef8115a80749ac47f295e9a70217a5553970c2b3"
   },
   "source": [
    "# Undersample Empty Images\n",
    "Here we undersample the empty images to get a better balanced group with more ships to try and segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20 2019-07-27 14:56:44 : Balanced data frame with 11000 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     500\n",
       "1     953\n",
       "2     547\n",
       "3     876\n",
       "4     624\n",
       "5     822\n",
       "6     678\n",
       "7     772\n",
       "8     728\n",
       "9     849\n",
       "10    651\n",
       "11    775\n",
       "12    725\n",
       "13    470\n",
       "14    536\n",
       "15    494\n",
       "Name: ships, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\n",
    "def sample_ships(in_df, base_rep_val=1500):\n",
    "    if in_df['ships'].values[0]==0:\n",
    "        return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n",
    "    else:\n",
    "        return in_df.sample(base_rep_val, replace=(in_df.shape[0]<base_rep_val))\n",
    "\n",
    "balanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\n",
    "balanced_train_df['ships'].hist(bins=np.arange(10))\n",
    "logging.info(\"Balanced data frame with {} records\".format(len(balanced_train_df)))\n",
    "# plt.show()\n",
    "balanced_train_df['ships'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode all the RLEs into Images\n",
    "We make a generator to produce batches of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_gen(in_df, batch_size = BATCH_SIZE):\n",
    "    all_batches = list(in_df.groupby('ImageId'))\n",
    "    out_rgb = []\n",
    "    out_mask = []\n",
    "    while True:\n",
    "        np.random.shuffle(all_batches)\n",
    "        for c_img_id, c_masks in all_batches:\n",
    "            rgb_path = DIR_IMAGES / c_img_id\n",
    "            c_img = imread(rgb_path)\n",
    "            c_mask = masks_as_image(c_masks['EncodedPixels'].values)\n",
    "            if IMG_SCALING is not None:\n",
    "                c_img = c_img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "                c_mask = c_mask[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "            out_rgb += [c_img]\n",
    "            out_mask += [c_mask]\n",
    "            if len(out_rgb)>=batch_size:\n",
    "                yield np.stack(out_rgb, 0)/255.0, np.stack(out_mask, 0)\n",
    "                out_rgb, out_mask=[], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (8, 768, 768, 3) 0.0 1.0\n",
      "y (8, 768, 768, 1) 0 1\n"
     ]
    }
   ],
   "source": [
    "train_gen = make_image_gen(balanced_train_df)\n",
    "# Get a single sample\n",
    "train_x, train_y = next(train_gen)\n",
    "print('x', train_x.shape, train_x.min(), train_x.max())\n",
    "print('y', train_y.shape, train_y.min(), train_y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_PLOT:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (30, 10))\n",
    "    batch_rgb = montage_rgb(train_x)\n",
    "    batch_seg = montage(train_y[:, :, :, 0])\n",
    "    ax1.imshow(batch_rgb)\n",
    "    ax1.set_title('Images')\n",
    "    ax2.imshow(batch_seg)\n",
    "    ax2.set_title('Segmentations')\n",
    "    ax3.imshow(mark_boundaries(batch_rgb, \n",
    "                               batch_seg.astype(int)))\n",
    "    ax3.set_title('Outlined Ships')\n",
    "    plt.show()\n",
    "    # fig.savefig('overview.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 768, 768, 3) (400, 768, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\n",
    "print(valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "dg_args = dict(featurewise_center = False,\n",
    "                  samplewise_center = False,\n",
    "                  rotation_range = 15, \n",
    "                  width_shift_range = 0.1, \n",
    "                  height_shift_range = 0.1, \n",
    "                  shear_range = 0.01,\n",
    "                  zoom_range = [0.9, 1.25],  \n",
    "                  horizontal_flip = True, \n",
    "                  vertical_flip = True,\n",
    "                  fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last')\n",
    "# brightness can be problematic since it seems to change the labels differently from the images \n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args[' brightness_range'] = [0.5, 1.5]\n",
    "image_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args.pop('brightness_range')\n",
    "label_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "def create_aug_gen(in_gen, seed = None):\n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    for in_x, in_y in in_gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n",
    "        g_x = image_gen.flow(255*in_x, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        g_y = label_gen.flow(in_y, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "\n",
    "        yield next(g_x)/255.0, next(g_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (8, 768, 768, 3) float32 0.0 1.0\n",
      "y (8, 768, 768, 1) float32 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "cur_gen = create_aug_gen(train_gen)\n",
    "t_x, t_y = next(cur_gen)\n",
    "print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n",
    "# only keep first 9 samples to examine in detail\n",
    "t_x = t_x[:9]\n",
    "t_y = t_y[:9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_PLOT:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "    ax1.imshow(montage_rgb(t_x), cmap='gray')\n",
    "    ax1.set_title('images')\n",
    "    ax2.imshow(montage(t_y[:, :, :, 0]), cmap='gray_r')\n",
    "    ax2.set_title('ships')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Model\n",
    "Here we use a slight deviation on the U-Net standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "RGB_Input (InputLayer)          (None, 768, 768, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 768, 768, 3)  0           RGB_Input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 768, 768, 3)  12          gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 768, 768, 8)  224         batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 768, 768, 8)  584         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 384, 384, 8)  0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 384, 384, 16) 1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 384, 384, 16) 2320        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 192, 192, 16) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 192, 192, 32) 4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 192, 192, 32) 9248        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 96, 96, 32)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 96, 96, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 96, 96, 64)   36928       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 48, 48, 64)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 48, 48, 128)  73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 48, 48, 128)  147584      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 96, 96, 128)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96, 96, 192)  0           up_sampling2d[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 96, 96, 64)   110656      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 96, 96, 64)   36928       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 192, 192, 64) 0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 192, 192, 96) 0           up_sampling2d_1[0][0]            \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 192, 192, 32) 27680       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 192, 192, 32) 9248        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 384, 384, 32) 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384, 384, 48) 0           up_sampling2d_2[0][0]            \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 384, 384, 16) 6928        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 384, 384, 16) 2320        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 768, 768, 16) 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 768, 768, 24) 0           up_sampling2d_3[0][0]            \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 768, 768, 8)  1736        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 768, 768, 8)  584         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 768, 768, 1)  9           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 736, 736, 1)  0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 768, 768, 1)  0           cropping2d[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 491,149\n",
      "Trainable params: 491,143\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build U-Net model\n",
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "\n",
    "def upsample_simple(filters, kernel_size, strides, padding):\n",
    "    return layers.UpSampling2D(strides)\n",
    "\n",
    "if UPSAMPLE_MODE=='DECONV':\n",
    "    upsample=upsample_conv\n",
    "else:\n",
    "    upsample=upsample_simple\n",
    "    \n",
    "input_img = layers.Input(t_x.shape[1:], name = 'RGB_Input')\n",
    "pp_in_layer = input_img\n",
    "if NET_SCALING is not None:\n",
    "    pp_in_layer = layers.AvgPool2D(NET_SCALING)(pp_in_layer)\n",
    "    \n",
    "pp_in_layer = layers.GaussianNoise(GAUSSIAN_NOISE)(pp_in_layer)\n",
    "pp_in_layer = layers.BatchNormalization()(pp_in_layer)\n",
    "\n",
    "c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (pp_in_layer)\n",
    "c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = layers.MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "p2 = layers.MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "p3 = layers.MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "p4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "\n",
    "c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "u6 = upsample(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = layers.concatenate([u6, c4])\n",
    "c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "u7 = upsample(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = layers.concatenate([u7, c3])\n",
    "c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "u8 = upsample(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = layers.concatenate([u8, c2])\n",
    "c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "u9 = upsample(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = layers.concatenate([u9, c1], axis=3)\n",
    "c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "d = layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "d = layers.Cropping2D((EDGE_CROP, EDGE_CROP))(d)\n",
    "d = layers.ZeroPadding2D((EDGE_CROP, EDGE_CROP))(d)\n",
    "if NET_SCALING is not None:\n",
    "    d = layers.UpSampling2D(NET_SCALING)(d)\n",
    "\n",
    "seg_model = models.Model(inputs=[input_img], outputs=[d])\n",
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n",
    "seg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 2019-07-27 14:57:48 : `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "weight_path = DIR_WEIGHTS / \"{}_weights.best.hdf5\".format('seg_model')\n",
    "weight_path = str(weight_path)\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n",
    "                                   patience=3, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor=\"val_dice_coef\", \n",
    "                      mode=\"max\", \n",
    "                      patience=15) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential step size: 1375\n",
      "Actual steps per epoch: 200\n"
     ]
    }
   ],
   "source": [
    "print('Potential step size:', balanced_train_df.shape[0]//BATCH_SIZE)\n",
    "step_count = min(MAX_TRAIN_STEPS, balanced_train_df.shape[0]//BATCH_SIZE)\n",
    "print('Actual steps per epoch:', step_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "199/200 [============================>.] - ETA: 16s - loss: -0.1682 - dice_coef: 0.0226 - binary_accuracy: 0.5460 - true_positive_rate: 0.7152"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 2019-07-27 15:56:54 : This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc3ebfb5da0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_dice_coef improved from -inf to 0.29849, saving model to /home/ubuntu/DATA/weights/seg_model_weights.best.hdf5\n",
      "200/200 [==============================] - 3539s 18s/step - loss: -0.1726 - dice_coef: 0.0232 - binary_accuracy: 0.5483 - true_positive_rate: 0.7117 - val_loss: -2.3877 - val_dice_coef: 0.2985 - val_binary_accuracy: 0.9988 - val_true_positive_rate: nan\n",
      "Epoch 2/30\n",
      "199/200 [============================>.] - ETA: 16s - loss: -0.9954 - dice_coef: 0.1244 - binary_accuracy: 0.9968 - true_positive_rate: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 2019-07-27 16:55:37 : This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7fc3ebfb5da0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_dice_coef improved from 0.29849 to 0.66950, saving model to /home/ubuntu/DATA/weights/seg_model_weights.best.hdf5\n",
      "200/200 [==============================] - 3523s 18s/step - loss: -0.9905 - dice_coef: 0.1238 - binary_accuracy: 0.9968 - true_positive_rate: 0.0000e+00 - val_loss: -5.3548 - val_dice_coef: 0.6695 - val_binary_accuracy: 0.9988 - val_true_positive_rate: nan\n",
      "Epoch 3/30\n",
      " 76/200 [==========>...................] - ETA: 33:16 - loss: -0.8022 - dice_coef: 0.1003 - binary_accuracy: 0.9967 - true_positive_rate: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "aug_gen = create_aug_gen(make_image_gen(balanced_train_df))\n",
    "loss_history = [seg_model.fit_generator(aug_gen, \n",
    "                             steps_per_epoch=step_count, \n",
    "                             epochs=NB_EPOCHS, \n",
    "                             validation_data=(valid_x, valid_y),\n",
    "                             callbacks=callbacks_list,\n",
    "                            workers=1 # the generator is not very thread safe\n",
    "                                       )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss(loss_history):\n",
    "    epich = np.cumsum(np.concatenate(\n",
    "        [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 10))\n",
    "    _ = ax1.plot(epich,\n",
    "                 np.concatenate([mh.history['loss'] for mh in loss_history]),\n",
    "                 'b-',\n",
    "                 epich, np.concatenate(\n",
    "            [mh.history['val_loss'] for mh in loss_history]), 'r-')\n",
    "    ax1.legend(['Training', 'Validation'])\n",
    "    ax1.set_title('Loss')\n",
    "\n",
    "    _ = ax2.plot(epich, np.concatenate(\n",
    "        [mh.history['true_positive_rate'] for mh in loss_history]), 'b-',\n",
    "                     epich, np.concatenate(\n",
    "            [mh.history['val_true_positive_rate'] for mh in loss_history]),\n",
    "                     'r-')\n",
    "    ax2.legend(['Training', 'Validation'])\n",
    "    ax2.set_title('True Positive Rate\\n(Positive Accuracy)')\n",
    "    \n",
    "    _ = ax3.plot(epich, np.concatenate(\n",
    "        [mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n",
    "                     epich, np.concatenate(\n",
    "            [mh.history['val_binary_accuracy'] for mh in loss_history]),\n",
    "                     'r-')\n",
    "    ax3.legend(['Training', 'Validation'])\n",
    "    ax3.set_title('Binary Accuracy (%)')\n",
    "    \n",
    "    _ = ax4.plot(epich, np.concatenate(\n",
    "        [mh.history['dice_coef'] for mh in loss_history]), 'b-',\n",
    "                     epich, np.concatenate(\n",
    "            [mh.history['val_dice_coef'] for mh in loss_history]),\n",
    "                     'r-')\n",
    "    ax4.legend(['Training', 'Validation'])\n",
    "    ax4.set_title('DICE')\n",
    "\n",
    "show_loss(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce1167e9f09200f537e61f93f486168a13be1711"
   },
   "outputs": [],
   "source": [
    "seg_model.load_weights(weight_path)\n",
    "seg_model.save('seg_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fe53610c198>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "275b411dc97a350aacaba46c8562efcf2658b1a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 768, 768, 1) 0.0 0.0023689687 2.5622537e-08\n"
     ]
    }
   ],
   "source": [
    "pred_y = seg_model.predict(valid_x, batch_size=4)\n",
    "print(pred_y.shape, pred_y.min(), pred_y.max(), pred_y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y[50,:,:,0]\n",
    "pred_y[50,:,:,:].shape\n",
    "probs = pred_y[50,:,:,0].ravel()\n",
    "plt.hist(probs[probs > 0.1])\n",
    "#plt.hist(probs[probs > 0.1])\n",
    "plt.hist()\n",
    "pred_y[50,0,1]\n",
    "for i in range(10):\n",
    "    plt.imshow(pred_y[i,:,:,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "ax.hist(pred_y.ravel(), np.linspace(0, 1, 10))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yscale('log', nonposy='clip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Full Resolution Model\n",
    "Here we account for the scaling so everything can happen in the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMG_SCALING is not None:\n",
    "    fullres_model = models.Sequential()\n",
    "    fullres_model.add(layers.AvgPool2D(IMG_SCALING, input_shape = (None, None, 3)))\n",
    "    fullres_model.add(seg_model)\n",
    "    fullres_model.add(layers.UpSampling2D(IMG_SCALING))\n",
    "else:\n",
    "    fullres_model = seg_model\n",
    "fullres_model.save('fullres_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = os.listdir(test_image_dir)\n",
    "print(len(test_paths), 'test images found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(20, 2, figsize = (10, 40))\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "for (ax1, ax2), c_img_name in zip(m_axs, test_paths):\n",
    "    c_path = os.path.join(test_image_dir, c_img_name)\n",
    "    c_img = imread(c_path)\n",
    "    first_img = np.expand_dims(c_img, 0)/255.0\n",
    "    first_seg = fullres_model.predict(first_img)\n",
    "    ax1.imshow(first_img[0])\n",
    "    ax1.set_title('Image')\n",
    "    ax2.imshow(first_seg[0, :, :, 0], vmin = 0, vmax = 1)\n",
    "    ax2.set_title('Prediction')\n",
    "fig.savefig('test_predictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11a6c6615131ff8c317f95a5097b46565ef21121"
   },
   "source": [
    "# Submission\n",
    "Since gneerating the submission takes a long time and quite a bit of memory we run it in a seperate kernel located at https://www.kaggle.com/kmader/from-trained-u-net-to-submission-part-2 \n",
    "That kernel takes the model saved in this kernel and applies it to all the test data"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,_uuid,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "deep_nikolaj",
   "language": "python",
   "name": "deep_nikolaj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
